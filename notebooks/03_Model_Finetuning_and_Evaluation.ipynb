{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b918800",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fentahun/10_acadamy/EthioMart_NER_week-4/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n",
      "0it [00:00, ?it/s]\n",
      "Casting the dataset: 100%|██████████| 37/37 [00:00<00:00, 18683.99 examples/s]\n",
      "/home/fentahun/10_acadamy/EthioMart_NER_week-4/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset processed and split successfully:\n",
      " DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['tokens', 'ner_tags'],\n",
      "        num_rows: 29\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['tokens', 'ner_tags'],\n",
      "        num_rows: 8\n",
      "    })\n",
      "})\n",
      "\n",
      "Sample from training data:\n",
      " {'tokens': ['HP', 'PAVILION', 'Price', 'አድራሻ:', '-', 'መገናኛ', 'ማራቶን', 'የ', 'ገበያ', 'ማእከል'], 'ner_tags': [1, 2, 0, 0, 0, 5, 6, 6, 6, 6]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 29/29 [00:00<00:00, 3639.47 examples/s]\n",
      "Map: 100%|██████████| 8/8 [00:00<00:00, 1963.05 examples/s]\n",
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at Davlan/afro-xlmr-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting model training with expanded entity list...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]/home/fentahun/10_acadamy/EthioMart_NER_week-4/venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "                                               \n",
      "  4%|▍         | 4/100 [00:13<04:52,  3.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.0826213359832764, 'eval_precision': 0.4857142857142857, 'eval_recall': 0.425, 'eval_f1-score': 0.45333333333333337, 'eval_runtime': 0.2947, 'eval_samples_per_second': 27.145, 'eval_steps_per_second': 3.393, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \n",
      "  8%|▊         | 8/100 [00:26<04:34,  2.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5904695987701416, 'eval_precision': 0.6122448979591837, 'eval_recall': 0.75, 'eval_f1-score': 0.6741573033707865, 'eval_runtime': 0.4467, 'eval_samples_per_second': 17.91, 'eval_steps_per_second': 2.239, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 10/100 [00:32<04:55,  3.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0509, 'grad_norm': 5.453810691833496, 'learning_rate': 2.7000000000000002e-05, 'epoch': 2.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \n",
      " 12%|█▏        | 12/100 [00:39<04:30,  3.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.196598768234253, 'eval_precision': 0.7560975609756098, 'eval_recall': 0.775, 'eval_f1-score': 0.7654320987654322, 'eval_runtime': 0.4105, 'eval_samples_per_second': 19.488, 'eval_steps_per_second': 2.436, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \n",
      " 16%|█▌        | 16/100 [00:52<04:24,  3.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8953021764755249, 'eval_precision': 0.7560975609756098, 'eval_recall': 0.775, 'eval_f1-score': 0.7654320987654322, 'eval_runtime': 0.4124, 'eval_samples_per_second': 19.399, 'eval_steps_per_second': 2.425, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 20/100 [01:06<04:22,  3.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0779, 'grad_norm': 3.873598575592041, 'learning_rate': 2.4e-05, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \n",
      " 20%|██        | 20/100 [01:06<04:22,  3.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7161499261856079, 'eval_precision': 0.7441860465116279, 'eval_recall': 0.8, 'eval_f1-score': 0.7710843373493975, 'eval_runtime': 0.423, 'eval_samples_per_second': 18.91, 'eval_steps_per_second': 2.364, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \n",
      " 24%|██▍       | 24/100 [01:20<04:08,  3.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5566896200180054, 'eval_precision': 0.8095238095238095, 'eval_recall': 0.85, 'eval_f1-score': 0.8292682926829269, 'eval_runtime': 0.6102, 'eval_samples_per_second': 13.111, 'eval_steps_per_second': 1.639, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \n",
      " 28%|██▊       | 28/100 [01:32<03:42,  3.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.43937015533447266, 'eval_precision': 0.8780487804878049, 'eval_recall': 0.9, 'eval_f1-score': 0.888888888888889, 'eval_runtime': 0.3918, 'eval_samples_per_second': 20.42, 'eval_steps_per_second': 2.552, 'epoch': 7.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 30/100 [01:39<03:47,  3.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5857, 'grad_norm': 2.8827106952667236, 'learning_rate': 2.1e-05, 'epoch': 7.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \n",
      " 32%|███▏      | 32/100 [01:45<03:25,  3.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3294372260570526, 'eval_precision': 0.9743589743589743, 'eval_recall': 0.95, 'eval_f1-score': 0.9620253164556962, 'eval_runtime': 0.4964, 'eval_samples_per_second': 16.114, 'eval_steps_per_second': 2.014, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \n",
      " 36%|███▌      | 36/100 [01:57<03:11,  2.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.25700685381889343, 'eval_precision': 0.9743589743589743, 'eval_recall': 0.95, 'eval_f1-score': 0.9620253164556962, 'eval_runtime': 0.3886, 'eval_samples_per_second': 20.589, 'eval_steps_per_second': 2.574, 'epoch': 9.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 40/100 [02:09<02:58,  2.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2803, 'grad_norm': 2.309680461883545, 'learning_rate': 1.8e-05, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \n",
      " 40%|████      | 40/100 [02:10<02:58,  2.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.20938891172409058, 'eval_precision': 0.9743589743589743, 'eval_recall': 0.95, 'eval_f1-score': 0.9620253164556962, 'eval_runtime': 0.4635, 'eval_samples_per_second': 17.261, 'eval_steps_per_second': 2.158, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \n",
      " 44%|████▍     | 44/100 [02:22<02:47,  3.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.18921521306037903, 'eval_precision': 0.9743589743589743, 'eval_recall': 0.95, 'eval_f1-score': 0.9620253164556962, 'eval_runtime': 0.4651, 'eval_samples_per_second': 17.202, 'eval_steps_per_second': 2.15, 'epoch': 11.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \n",
      " 48%|████▊     | 48/100 [02:35<02:33,  2.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.18794341385364532, 'eval_precision': 0.9743589743589743, 'eval_recall': 0.95, 'eval_f1-score': 0.9620253164556962, 'eval_runtime': 0.3895, 'eval_samples_per_second': 20.539, 'eval_steps_per_second': 2.567, 'epoch': 12.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 50/100 [02:41<02:36,  3.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1468, 'grad_norm': 2.207854747772217, 'learning_rate': 1.5e-05, 'epoch': 12.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \n",
      " 52%|█████▏    | 52/100 [02:47<02:24,  3.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.1768486499786377, 'eval_precision': 0.9743589743589743, 'eval_recall': 0.95, 'eval_f1-score': 0.9620253164556962, 'eval_runtime': 0.3791, 'eval_samples_per_second': 21.103, 'eval_steps_per_second': 2.638, 'epoch': 13.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \n",
      " 56%|█████▌    | 56/100 [03:00<02:14,  3.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.17175722122192383, 'eval_precision': 0.9743589743589743, 'eval_recall': 0.95, 'eval_f1-score': 0.9620253164556962, 'eval_runtime': 0.3836, 'eval_samples_per_second': 20.854, 'eval_steps_per_second': 2.607, 'epoch': 14.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 60/100 [03:12<01:58,  2.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0954, 'grad_norm': 0.6211458444595337, 'learning_rate': 1.2e-05, 'epoch': 15.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \n",
      " 60%|██████    | 60/100 [03:12<01:58,  2.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.17363359034061432, 'eval_precision': 0.9743589743589743, 'eval_recall': 0.95, 'eval_f1-score': 0.9620253164556962, 'eval_runtime': 0.3946, 'eval_samples_per_second': 20.272, 'eval_steps_per_second': 2.534, 'epoch': 15.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \n",
      " 64%|██████▍   | 64/100 [03:25<01:47,  2.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.17316783964633942, 'eval_precision': 0.9743589743589743, 'eval_recall': 0.95, 'eval_f1-score': 0.9620253164556962, 'eval_runtime': 0.3952, 'eval_samples_per_second': 20.241, 'eval_steps_per_second': 2.53, 'epoch': 16.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \n",
      " 68%|██████▊   | 68/100 [03:37<01:35,  2.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.175831601023674, 'eval_precision': 0.9743589743589743, 'eval_recall': 0.95, 'eval_f1-score': 0.9620253164556962, 'eval_runtime': 0.384, 'eval_samples_per_second': 20.831, 'eval_steps_per_second': 2.604, 'epoch': 17.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 70/100 [03:44<01:36,  3.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0821, 'grad_norm': 0.9661248326301575, 'learning_rate': 9e-06, 'epoch': 17.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \n",
      " 72%|███████▏  | 72/100 [03:50<01:25,  3.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.18116715550422668, 'eval_precision': 0.9743589743589743, 'eval_recall': 0.95, 'eval_f1-score': 0.9620253164556962, 'eval_runtime': 0.3977, 'eval_samples_per_second': 20.117, 'eval_steps_per_second': 2.515, 'epoch': 18.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \n",
      " 76%|███████▌  | 76/100 [04:02<01:10,  2.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.18500234186649323, 'eval_precision': 0.9743589743589743, 'eval_recall': 0.95, 'eval_f1-score': 0.9620253164556962, 'eval_runtime': 0.3888, 'eval_samples_per_second': 20.576, 'eval_steps_per_second': 2.572, 'epoch': 19.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 80/100 [04:14<00:58,  2.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0527, 'grad_norm': 1.0591944456100464, 'learning_rate': 6e-06, 'epoch': 20.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \n",
      " 80%|████████  | 80/100 [04:15<00:58,  2.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.18638241291046143, 'eval_precision': 0.9743589743589743, 'eval_recall': 0.95, 'eval_f1-score': 0.9620253164556962, 'eval_runtime': 0.3789, 'eval_samples_per_second': 21.116, 'eval_steps_per_second': 2.64, 'epoch': 20.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \n",
      " 84%|████████▍ | 84/100 [04:27<00:48,  3.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.18689016997814178, 'eval_precision': 0.9743589743589743, 'eval_recall': 0.95, 'eval_f1-score': 0.9620253164556962, 'eval_runtime': 0.4381, 'eval_samples_per_second': 18.259, 'eval_steps_per_second': 2.282, 'epoch': 21.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \n",
      " 88%|████████▊ | 88/100 [04:40<00:37,  3.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.1885034590959549, 'eval_precision': 0.9743589743589743, 'eval_recall': 0.95, 'eval_f1-score': 0.9620253164556962, 'eval_runtime': 0.4669, 'eval_samples_per_second': 17.135, 'eval_steps_per_second': 2.142, 'epoch': 22.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 90/100 [04:48<00:34,  3.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0494, 'grad_norm': 0.41516947746276855, 'learning_rate': 3e-06, 'epoch': 22.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \n",
      " 92%|█████████▏| 92/100 [04:54<00:25,  3.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.18892742693424225, 'eval_precision': 0.9743589743589743, 'eval_recall': 0.95, 'eval_f1-score': 0.9620253164556962, 'eval_runtime': 0.3994, 'eval_samples_per_second': 20.031, 'eval_steps_per_second': 2.504, 'epoch': 23.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \n",
      " 96%|█████████▌| 96/100 [05:08<00:12,  3.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.1882058084011078, 'eval_precision': 0.9743589743589743, 'eval_recall': 0.95, 'eval_f1-score': 0.9620253164556962, 'eval_runtime': 0.4298, 'eval_samples_per_second': 18.611, 'eval_steps_per_second': 2.326, 'epoch': 24.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [05:20<00:00,  3.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0432, 'grad_norm': 0.46382230520248413, 'learning_rate': 0.0, 'epoch': 25.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      "100%|██████████| 100/100 [05:21<00:00,  3.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.18778850138187408, 'eval_precision': 0.9743589743589743, 'eval_recall': 0.95, 'eval_f1-score': 0.9620253164556962, 'eval_runtime': 0.5301, 'eval_samples_per_second': 15.093, 'eval_steps_per_second': 1.887, 'epoch': 25.0}\n",
      "{'train_runtime': 321.1992, 'train_samples_per_second': 2.257, 'train_steps_per_second': 0.311, 'train_loss': 0.4464517691731453, 'epoch': 25.0}\n",
      "\n",
      "Evaluating final model on the test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 184.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Evaluation results: {'eval_loss': 0.18778850138187408, 'eval_precision': 0.9743589743589743, 'eval_recall': 0.95, 'eval_f1-score': 0.9620253164556962, 'eval_runtime': 0.4634, 'eval_samples_per_second': 17.263, 'eval_steps_per_second': 2.158, 'epoch': 25.0}\n",
      "\n",
      "Model and tokenizer saved successfully to '../saved_models/amharic-ner-afro-xlmr'\n",
      "\n",
      "--- Testing the fine-tuned model with a pipeline ---\n",
      "Test text: 1 pairs Sneaker Crease Protector ዋጋ፦ 400 ብር አድራሻ መገናኛ ቢሮ ቁ. S05/S06 0902660722\n",
      "{'entity_group': 'PRODUCT', 'score': np.float32(0.99399006), 'word': '1 pairs Sneaker Crease Protector', 'start': 0, 'end': 32}\n",
      "{'entity_group': 'PRICE', 'score': np.float32(0.9858272), 'word': '400 ብር', 'start': 37, 'end': 43}\n",
      "{'entity_group': 'LOC', 'score': np.float32(0.87952137), 'word': 'መገናኛ ቢሮ ቁ. S05/S06', 'start': 49, 'end': 67}\n",
      "{'entity_group': 'CONTACT_INFO', 'score': np.float32(0.99241734), 'word': '09', 'start': 68, 'end': 70}\n",
      "{'entity_group': 'CONTACT_INFO', 'score': np.float32(0.9698542), 'word': '02', 'start': 70, 'end': 72}\n",
      "{'entity_group': 'CONTACT_INFO', 'score': np.float32(0.4168863), 'word': '660', 'start': 72, 'end': 75}\n",
      "{'entity_group': 'LOC', 'score': np.float32(0.21820126), 'word': '722', 'start': 75, 'end': 78}\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Task 3: Fine-Tuning a Transformer Model for Amharic NER\n",
    "# This version is updated to handle all specified entities, including optional ones.\n",
    "# =============================================================================\n",
    "\n",
    "import numpy as np\n",
    "from datasets import load_dataset, Dataset, DatasetDict, Features, Sequence, ClassLabel, Value\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, TrainingArguments, Trainer, DataCollatorForTokenClassification\n",
    "from seqeval.metrics import classification_report\n",
    "import os\n",
    "import torch\n",
    "\n",
    "# --- Step 1: Define Comprehensive Label List and Load Dataset ---\n",
    "# The label_list is expanded to include all possible entities from the project brief.\n",
    "# This is the key change to fix the previous \"Invalid string class label\" error.\n",
    "label_list = [\n",
    "    \"O\", \n",
    "    \"B-PRODUCT\", \"I-PRODUCT\", \n",
    "    \"B-PRICE\", \"I-PRICE\", \n",
    "    \"B-LOC\", \"I-LOC\",\n",
    "    \"B-CONTACT_INFO\", \"I-CONTACT_INFO\",\n",
    "    \"B-DELIVERY_FEE\", \"I-DELIVERY_FEE\"\n",
    "]\n",
    "labeled_file_path = '../data/labeled_data.txt'\n",
    "\n",
    "if not os.path.exists(labeled_file_path) or os.path.getsize(labeled_file_path) == 0:\n",
    "    raise FileNotFoundError(f\"Labeled data file not found or is empty at: {labeled_file_path}.\")\n",
    "\n",
    "# --- Step 2: Robustly Parse the CoNLL File into a Dataset ---\n",
    "def read_conll_file(file_path):\n",
    "    \"\"\"Reads a CoNLL-formatted file and converts it into a Hugging Face Dataset.\"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        tokens_list, ner_tags_list = [], []\n",
    "        tokens, ner_tags = [], []\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line == \"\":\n",
    "                if tokens:\n",
    "                    tokens_list.append(tokens)\n",
    "                    ner_tags_list.append(ner_tags)\n",
    "                    tokens, ner_tags = [], []\n",
    "            else:\n",
    "                parts = line.split()\n",
    "                if len(parts) == 2 and parts[1] in label_list:\n",
    "                    tokens.append(parts[0])\n",
    "                    ner_tags.append(parts[1])\n",
    "                # Silently ignore lines with invalid labels to prevent crashes\n",
    "        if tokens: # Add the last sentence\n",
    "            tokens_list.append(tokens)\n",
    "            ner_tags_list.append(ner_tags)\n",
    "    return Dataset.from_dict({'tokens': tokens_list, 'ner_tags': ner_tags_list})\n",
    "\n",
    "# Create and structure the dataset with the correct features\n",
    "raw_dataset = read_conll_file(labeled_file_path)\n",
    "features = Features({\n",
    "    'tokens': Sequence(Value('string')),\n",
    "    'ner_tags': Sequence(ClassLabel(names=label_list))\n",
    "})\n",
    "raw_dataset = raw_dataset.cast(features)\n",
    "\n",
    "# Split into training and test sets\n",
    "final_datasets = raw_dataset.train_test_split(test_size=0.2, seed=42)\n",
    "print(\"Dataset processed and split successfully:\\n\", final_datasets)\n",
    "print(\"\\nSample from training data:\\n\", final_datasets['train'][0])\n",
    "\n",
    "# --- Step 3: Tokenization and Label Alignment ---\n",
    "model_checkpoint = \"Davlan/afro-xlmr-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    \"\"\"Tokenizes sentences and aligns labels to the new sub-word tokens.\"\"\"\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"], \n",
    "        truncation=True, \n",
    "        is_split_into_words=True\n",
    "    )\n",
    "    all_aligned_labels = []\n",
    "    for i, labels_for_one_sentence in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        current_aligned_labels = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                current_aligned_labels.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                current_aligned_labels.append(labels_for_one_sentence[word_idx])\n",
    "            else:\n",
    "                current_aligned_labels.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        all_aligned_labels.append(current_aligned_labels)\n",
    "    tokenized_inputs[\"labels\"] = all_aligned_labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "tokenized_datasets = final_datasets.map(tokenize_and_align_labels, batched=True)\n",
    "\n",
    "# --- Step 4: Model Training ---\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_checkpoint, num_labels=len(label_list), id2label={i: l for i, l in enumerate(label_list)}, label2id={l: i for i, l in enumerate(label_list)}\n",
    ")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"../saved_models/amharic-ner-afro-xlmr\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=3e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=25, # Increased epochs for more complex entities\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=10,\n",
    ")\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "\n",
    "def compute_metrics(p):\n",
    "    \"\"\"Computes precision, recall, and F1-score for the evaluation set.\"\"\"\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(pred, lab) if l != -100]\n",
    "        for pred, lab in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(pred, lab) if l != -100]\n",
    "        for pred, lab in zip(predictions, labels)\n",
    "    ]\n",
    "    report = classification_report(true_labels, true_predictions, output_dict=True, zero_division=0)\n",
    "    return {\n",
    "        \"precision\": report[\"micro avg\"][\"precision\"],\n",
    "        \"recall\": report[\"micro avg\"][\"recall\"],\n",
    "        \"f1-score\": report[\"micro avg\"][\"f1-score\"],\n",
    "    }\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print(\"Starting model training with expanded entity list...\")\n",
    "trainer.train()\n",
    "\n",
    "# --- Step 5: Final Evaluation and Saving ---\n",
    "print(\"\\nEvaluating final model on the test set...\")\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\"Final Evaluation results: {eval_results}\")\n",
    "\n",
    "model_save_path = \"../saved_models/amharic-ner-afro-xlmr\"\n",
    "trainer.save_model(model_save_path)\n",
    "tokenizer.save_pretrained(model_save_path)\n",
    "print(f\"\\nModel and tokenizer saved successfully to '{model_save_path}'\")\n",
    "\n",
    "# --- Step 6: Test the Final Model on a Complex Example ---\n",
    "print(\"\\n--- Testing the fine-tuned model with a pipeline ---\")\n",
    "from transformers import pipeline\n",
    "\n",
    "ner_pipeline = pipeline(\"ner\", model=model_save_path, aggregation_strategy=\"simple\")\n",
    "text = \"1 pairs Sneaker Crease Protector ዋጋ፦ 400 ብር አድራሻ መገናኛ ቢሮ ቁ. S05/S06 0902660722\"\n",
    "results = ner_pipeline(text)\n",
    "print(f\"Test text: {text}\")\n",
    "for entity in results:\n",
    "    print(entity)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
