{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "363baa0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import asyncio\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from telethon.sync import TelegramClient\n",
    "from telethon.tl.functions.messages import GetHistoryRequest\n",
    "from telethon.errors.rpcerrorlist import ChannelInvalidError, ChannelPrivateError\n",
    "import nest_asyncio\n",
    "\n",
    "# Apply nest_asyncio to allow running asyncio event loops within a Jupyter notebook\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3bbf8c4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading credentials from .env file...\n",
      "Credentials loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading credentials from .env file...\")\n",
    "load_dotenv()\n",
    "\n",
    "# Get credentials securely from the environment\n",
    "api_id = os.getenv(\"API_ID\")\n",
    "api_hash = os.getenv(\"API_HASH\")\n",
    "phone = os.getenv(\"PHONE_NUMBER\")\n",
    "\n",
    "# Check if the variables were loaded correctly\n",
    "if not all([api_id, api_hash, phone]):\n",
    "    raise ValueError(\"API_ID, API_HASH, or PHONE_NUMBER not found. Make sure you have a .env file in the project root with the correct values.\")\n",
    "\n",
    "print(\"Credentials loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2754934",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "channels_to_scrape = [\n",
    "    'ZemenExpress',\n",
    "    'nevacomputer',\n",
    "    'kuruwear',\n",
    "    'helloomarketethiopia',\n",
    "    'Shewabrand'\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d732a080",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def fetch_messages(client, channel_identifier, limit=300):\n",
    "    \"\"\"\n",
    "    Asynchronously fetches message history from a single Telegram channel using its ID or username.\n",
    "    \"\"\"\n",
    "    all_messages = []\n",
    "    channel_name_for_print = str(channel_identifier) # Default name for logging\n",
    "\n",
    "    try:\n",
    "        # This works whether you provide a username or an integer ID\n",
    "        channel_entity = await client.get_entity(channel_identifier)\n",
    "        channel_name_for_print = getattr(channel_entity, 'username', str(channel_identifier))\n",
    "        \n",
    "        async for message in client.iter_messages(channel_entity, limit=limit):\n",
    "            # We only care about messages with text content\n",
    "            if message.text:\n",
    "                all_messages.append({\n",
    "                    'channel_name': getattr(channel_entity, 'username', str(channel_identifier)),\n",
    "                    'message_id': message.id,\n",
    "                    'timestamp': message.date,\n",
    "                    'text_original': message.text,\n",
    "                    'views': message.views if message.views else 0,\n",
    "                    'has_image': message.photo is not None\n",
    "                })\n",
    "        \n",
    "        print(f\"Successfully scraped {len(all_messages)} messages from @{channel_name_for_print}.\")\n",
    "        return all_messages\n",
    "\n",
    "    except (ChannelInvalidError, ValueError):\n",
    "        print(f\"Error: Channel '@{channel_name_for_print}' not found or is invalid. Skipping.\")\n",
    "        return []\n",
    "    except ChannelPrivateError:\n",
    "        print(f\"Error: Channel '@{channel_name_for_print}' is private. You must join it first. Skipping.\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred with @{channel_name_for_print}: {e}\")\n",
    "        return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bac91470",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def main():\n",
    "    \"\"\"Main function to coordinate the scraping process.\"\"\"\n",
    "    all_scraped_data = []\n",
    "    \n",
    "    # The 'async with' block handles connecting and disconnecting automatically\n",
    "    # The session file will be created in the root directory.\n",
    "    async with TelegramClient('session_name', api_id, api_hash) as client:\n",
    "        print(\"Client created successfully. Starting to scrape channels...\")\n",
    "        for channel in channels_to_scrape:\n",
    "            print(f\"--- Processing channel: {channel} ---\")\n",
    "            channel_data = await fetch_messages(client, channel, limit=300) # Fetch up to 300 messages per channel\n",
    "            all_scraped_data.extend(channel_data)\n",
    "\n",
    "    if not all_scraped_data:\n",
    "        print(\"\\nScraping finished, but no data was collected. Please check your channel IDs/usernames and network.\")\n",
    "        return pd.DataFrame() # Return an empty dataframe\n",
    "    \n",
    "    df = pd.DataFrame(all_scraped_data)\n",
    "    \n",
    "    # Save the raw scraped data before any cleaning\n",
    "    df.to_csv('../data/scraped_data.csv', index=False, encoding='utf-8-sig')\n",
    "    print(f\"\\nScraping complete. Collected a total of {len(df)} messages.\")\n",
    "    print(\"Raw data saved to 'data/scraped_data.csv'\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eabdc8aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting the scraping process...\n",
      "Client created successfully. Starting to scrape channels...\n",
      "--- Processing channel: ZemenExpress ---\n",
      "Successfully scraped 120 messages from @ZemenExpress.\n",
      "--- Processing channel: nevacomputer ---\n",
      "Successfully scraped 92 messages from @nevacomputer.\n",
      "--- Processing channel: kuruwear ---\n",
      "Successfully scraped 163 messages from @kuruwear.\n",
      "--- Processing channel: helloomarketethiopia ---\n",
      "Successfully scraped 276 messages from @helloomarketethiopia.\n",
      "--- Processing channel: Shewabrand ---\n",
      "Successfully scraped 246 messages from @Shewabrand.\n",
      "\n",
      "Scraping complete. Collected a total of 897 messages.\n",
      "Raw data saved to 'data/scraped_data.csv'\n",
      "\n",
      "--- Starting Data Preprocessing ---\n",
      "Cleaning text data...\n",
      "Preprocessing complete. Cleaned data saved to 'data/preprocessed_data.csv'\n",
      "\n",
      "--- Preprocessed DataFrame Sample ---\n",
      "Top 5 rows:\n",
      "   channel_name                                       text_cleaned  views\n",
      "0  ZemenExpress  ................................... üéØ LCD Writ...    441\n",
      "1  ZemenExpress  ................................... üéØ LCD Writ...    345\n",
      "2  ZemenExpress  ................................... üéØ LCD Writ...    369\n",
      "3  ZemenExpress  üëÄ ........... Electric Charcoal Burner üëç·â†·âÄ·àã·àâ ·ä®...   2039\n",
      "4  ZemenExpress  üëÄ ........... Electric Charcoal Burner üëç·â†·âÄ·àã·àâ ·ä®...   1659\n",
      "\n",
      "Random 5 rows:\n",
      "             channel_name                                       text_cleaned  \\\n",
      "340              kuruwear  100% ·äï·çÅ ·âÜ·ã≥ ·ã®·â∞·à∞·à© ·å´·àõ·ãé·âΩ üá™üáπ ·ã®·àÄ·åà·à´·âΩ·äï ·àù·à≠·âµ üá™üáπ 1400 ·â•·à≠ ...   \n",
      "763            Shewabrand  Buy Lymio cargo MADE IN TURKEY Size 29 SHEWA B...   \n",
      "820            Shewabrand  adidas SAMBA OG Size 40 MADE IN VIETNAM SHEWA ...   \n",
      "388  helloomarketethiopia  ·àà·â§·âµ·ãé ·ã∞·àù·âÄ·âµ ·ã®·àö·å®·àù·à© ·àª·àõ·ãé·âΩ ·â†·â∞·àà·ã´·ã® ·àò·å†·äï ·ä•·äì ·âÄ·àà·àù ·àà·àõ·ãò·ãù 097...   \n",
      "657            Shewabrand  adidas FEAR OF GOD original üíØ Size 36 MADE IN ...   \n",
      "\n",
      "     views  \n",
      "340   2913  \n",
      "763  11486  \n",
      "820  16696  \n",
      "388   3823  \n",
      "657   4308  \n",
      "\n",
      "Last 5 rows:\n",
      "    channel_name                                       text_cleaned  views\n",
      "892   Shewabrand  NIKE AIR JUST DO IT 2024 Size 40 MADE IN VIETN...  18226\n",
      "893   Shewabrand  AIR JORDAN 1 LOW Size 36 MADE IN VIETNAM SHEWA...  20284\n",
      "894   Shewabrand  AIR JORDAN 1 RETRO Size 36 MADE IN VIETNAM SHE...  19767\n",
      "895   Shewabrand  NAKED WOLFE Size 39 MADE IN VIETNAM SHEWA BRAN...  19741\n",
      "896   Shewabrand  NIKE Blazer Mid Size 40 MADE IN VIETNAM SHEWA ...  21452\n"
     ]
    }
   ],
   "source": [
    "# --- Step 6: Run the Scraper ---\n",
    "print(\"Starting the scraping process...\")\n",
    "# Note: The first time you run this, Telethon will ask for your phone number,\n",
    "# a login code sent to your Telegram app, and possibly your 2FA password.\n",
    "df_scraped = asyncio.run(main())\n",
    "\n",
    "if not df_scraped.empty:\n",
    "    print(\"\\n--- Starting Data Preprocessing ---\")\n",
    "\n",
    "    def clean_text(text):\n",
    "        \"\"\"A function to clean Amharic text for NER.\"\"\"\n",
    "        if not isinstance(text, str):\n",
    "            return \"\"\n",
    "        # Remove URLs and Telegram links\n",
    "        text = re.sub(r'http\\S+|www\\S+|t\\.me/\\S+', '', text, flags=re.MULTILINE)\n",
    "        # Remove user mentions and hashtags\n",
    "        text = re.sub(r'@\\w+|#\\w+', '', text)\n",
    "        # Remove specific decorative characters and common emojis\n",
    "        text = re.sub(r'[üí•üìåüíµ‚úÖüëâüìçüìû‚òéÔ∏èüëá‚ú®‚úî¬Æ¬©‚Ñ¢‚ù§üî•]', '', text)\n",
    "        # Replace multiple newlines/whitespace with a single space\n",
    "        text = re.sub(r'[\\n\\r\\s]+', ' ', text).strip()\n",
    "        return text\n",
    "\n",
    "    print(\"Cleaning text data...\")\n",
    "    df_scraped['text_cleaned'] = df_scraped['text_original'].apply(clean_text)\n",
    "    \n",
    "    # Save the final preprocessed data\n",
    "    df_scraped.to_csv('../data/preprocessed_data.csv', index=False, encoding='utf-8-sig')\n",
    "    print(\"Preprocessing complete. Cleaned data saved to 'data/preprocessed_data.csv'\")\n",
    "    \n",
    "    print(\"\\n--- Preprocessed DataFrame Sample ---\")\n",
    "    # Display a sample from different parts of the dataframe to see the variety\n",
    "    print(\"Top 5 rows:\")\n",
    "    print(df_scraped[['channel_name', 'text_cleaned', 'views']].head())\n",
    "    print(\"\\nRandom 5 rows:\")\n",
    "    print(df_scraped[['channel_name', 'text_cleaned', 'views']].sample(5))\n",
    "    print(\"\\nLast 5 rows:\")\n",
    "    print(df_scraped[['channel_name', 'text_cleaned', 'views']].tail())\n",
    "else:\n",
    "    print(\"\\nSkipping preprocessing because no data was scraped.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
